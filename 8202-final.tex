%&pdflatex
\documentclass[11pt]{article}

\usepackage{main-macros}


\usepackage[margin=0.75in]{geometry}


\title{8202 Final}
\author{Devon Tuma}
\date{Spring 2020}

\begin{document}
\maketitle

\problem{1}
\subproblem{a: true}
\begin{proof}
  If the Galois group is isomorphic to $V_4$, then $\K$ is of the form $\Q(\sqrt{D_1},\sqrt{D_2})$ for some $D_1,D_2 \in \Q$ with the property that none of $D_1$, $D_2$, or $D_1D_2$ is a square in $\Q$ (Exercise 14.2.15 in D \& F, I think we also showed in class at one point).
  Then consider the following polynomial (where I just picked the roots to make the resolvent cubic split completely):
  \begin{equation*}
    f(x) = (x + \sqrt{D_1} + \sqrt{D_2})(x - \sqrt{D_1} + \sqrt{D_2})(x + \sqrt{D_1} - \sqrt{D_2})(x - \sqrt{D_1} - \sqrt{D_2})
  \end{equation*}
  This polynomial is an element of $\Q[x]$, since it expands out to $f(x) = D_1^2 - 2 D_1 D_2 + D_2^2 - 2 D_1 x^2 - 2 D_2 x^2 + x^4$.
  It is also irreducible over $\Q$ since none of the linear or quadratic factors have coefficients in $\Q$
  (I just checked this manually for all $6$ quadratic factors with Sage).
  Finally, it's splitting field is precisely $\K$, since it clearly splits over $\K$, and also doesn't split over any of the three subfields $\Q(\sqrt{D_1})$, $\Q(\sqrt{D_2})$, $\Q(\sqrt{D_1D_2})$ (which are the only subfields since $V_4$ has only $3$ subgroups).
\end{proof}

\subproblem{b: true}
\begin{proof}
  Again for the same reasons we have that $\K$ is of the form $\Q(\sqrt{D_1},\sqrt{D_2})$ with the same conditions as before.
  Take $g(x) = (x^2 - D_1)(x^2 - D_2)$, which is reducible since it is a product of quadratic factors.
  Then $\K$ is the splitting field for $g$, since it clearly splits over $\K$, and doesn't split over any of the three subfields $\Q(\sqrt{D_1})$, $\Q(\sqrt{D_2})$, $\Q(\sqrt{D_1D_2})$ (which are the only subfields since $V_4$ has only $3$ subgroups).
\end{proof}

\subproblem{c: true}
\begin{proof}
  The smith normal form and greatest common divisor are both unique up to units, so it suffices to check for one possible smith normal form.
  In particular it suffices to check the smith normal form given by the algorithm described in class.
  Note that at the end of the algorithm $d_1 = gcd(d_1,\dots,d_r)$, since it's a common divisor by construction and there can't be a greater divisor since the $gcd$ of the $d_i$ must in particular divide $d_1$.
  Therefore it suffices to check that the $gcd$ is invariant under every step of the algorithm, since then $d_1$ is also the $gcd$ of the entries in the original matrix.

  But note that every step of the algorithm corresponds to a set of row and column operations, so it suffices to check these preserve the $gcd$.
  First the operation of swapping rows or columns preserves $gcd$ since the set of entries remains unchanged.
  Multiplication by a unit also preserves the $gcd$ since the $gcd$ is only unique up to units.
  Adding one row to another preserves $gcd$ as well, since $gcd(a,b) = gcd(a,a+b)$, and so the pairwise $gcd$ of the entries remains unchanged.
  Finally adding one column to another preserves $gcd$ for the same reason as adding rows.
  Therefore we conclude the algorithm also preserves $gcd$, and hence $d_1$ is the $gcd$ of all entries in the original matrix.
\end{proof}

\subproblem{d: false}
\begin{proof}
  The discriminant of $x^3 - 13x + 13$ is $D = 4(13)^3 - 27(13)^2 = 4225 = 65^2$, so the discriminant is a square in $\Q$.
  We saw in class that this implies that $Gal(\K/\Q) \le A_3$, so the Galois group can't possibly be $S_3$.
\end{proof}

\subproblem{e: true}
\begin{proof}
  First, note that there is no solution in the integers, since the left hand side is always positive for $a,b,c,d \in \Q$.
  Then assume for contradiction we have a solution $a_0,b_0,c_0,d_0 \in \K$.
  Since there are no solutions in $\Q$, and the equation is symmetric, we can assume WLOG that $a_0 \not \in \Q$ by rearranging variables.
  Then the polynomial $f(a) = a^2 + b_0^2 + c_0^2 + d_0^2 + 1$ must split completely in $\K$ since it has roots $\pm a_0$.
  But $a_0 \not \in \Q$, so this is the smallest field for which $f$ splits, and hence the $\K$ is the splitting field for $f$.
  Furthermore $f$ is separable since it has distinct roots, so $\K = \Q(\omega\sqrt[3]{2})$ is then a Galois extension.
  But we saw in class that this extension isn't Galois, so this is a contradiction, and we conclude that there is no solution in $\K$.
\end{proof}

\subproblem{f: true}
\begin{proof}
  Both have the same invariant factor form, namely $\Z/(2*3*5*7*11*13)\Z$. Therefore both groups are isomorphic to this one, and composing the respective isomorphisms to the invariant factor form gives an isomorphism between the original groups.
\end{proof}

\subproblem{g: true}
\begin{proof}
  If $A$ satisfies $A^5 = A$ then it's minimal polynomial divides $x^5 - x$.
  But this polynomial has $0,1,2,3,4$ as roots by Fermat's little theorem, and has at most $5$ roots by its degree, so it has distinct roots.
  Therefore the minimal polynomial also has distinct roots, and hence $A$ is diagonalizable.
\end{proof}

\subproblem{h: false}
\begin{proof}
  consider the matrix
  \begin{equation*}
    A = \begin{bmatrix}
      1 & 1 \\ 0 & 1
    \end{bmatrix}
  \end{equation*}
  When we work over $\F_2$ we have
  \begin{equation*}
    A^5 = \begin{bmatrix}
      1 & 1 \\ 0 & 1
    \end{bmatrix}^5
    = \begin{bmatrix}
      1 & 5 \\ 0 & 1
    \end{bmatrix}
    = \begin{bmatrix}
      1 & 1 \\ 0 & 1
    \end{bmatrix} = A
  \end{equation*}
  However any eigenvector for $A$ must be of the form $v = \begin{bmatrix} a & 0 \end{bmatrix}$, since if the second entry was non-zero then $A$ would fix the second entry but not the first, and $Av$ wouldn't be a multiple of $v$.
  Therefore we can't possibly find a basis of eigenvectors, since any two eigenvectors are linearly dependent.
  Thus we conclude $A$ is not diagonalizable even though $A^5 = A$.
\end{proof}


\problem{2}
\subproblem{a: 3}
\begin{proof}
  We know that the $\R[x]$-submodules of $V$ are in bijection with the subspaces of $V$ stable under the linear transformation for this matrix.
  All non-trivial sub-spaces will have dimension one, so we only need to count the stable one-dimensional subspaces.
  But note that elements of a one-dimensional subspace are all scalar multiples of each other,
  and this matrix sends $\begin{bmatrix}a & b\end{bmatrix}$ to $\begin{bmatrix}b & a\end{bmatrix}$,
  so the only stable subspace is the one generated by $\begin{bmatrix}1 & 1\end{bmatrix}$.
  This gives exactly one corresponding submodule, and so including the two trivial submodules we conclude there are three in total.
\end{proof}

\subproblem{b: $2^n$}
\begin{proof}
  Again this is the same as counting the number of $A$ stable subspaces of $V$.
  First note that the roots of the characteristic polynomial $\det(xI - A) = x^n - 1$ are exactly the $n$th roots of unity.
  Then these roots are also the eigenvalues of $A$, so $A$ has $n$ distinct eigenvalues and hence is diagonalizable.
  In particular $V$ has a basis consisting of $n$ distinct linearly independent eigenvectors of $A$, call them $v_1,\dots,v_n$.
  But note that any set of eigenvectors generates a stable subspace, since $A$ just acts by scaling the generating eigenvectors.
  So the number of stable subspaces is at least the number of subsets of these $n$ independent eigenvectors.
  
  We then claim there can't be any more stable subspaces than this.
  Given a stable subspace $W$, we know that we can find a basis of eigenvectors for it, say $w_1,\dots,w_k$.
  Then $w_i$ must be a linear combination of the $v_1,\dots,v_n$, since they were a basis for all of $V$.
  But each of those eigenvectors were independent and had distinct eigenvalues, so by linearity $w_i$ can't be an eigenvector if it is a combination of more than one of them.
  Hence $w_i = kv_j$ for one of the $v_j$, and this holds for all of the $w_i$, and hence $W$ is generated by a subset of the $v_1,\dots,v_n$.
  Therefore the number of stable subspaces is at most the number of subsets of the previous $n$ eigenvectors.
  
  Finally then the number of $\C[x]$-submodules is the number of subsets of $n$ elements, which is $2^n$.
\end{proof}

\problem{3: 3}
\begin{proof}
  First, matrices with the same rational canonical form are by construction in the same similarity class.
  Furthermore we saw that each matrix has a unique rational canonical form, so this covers every possible similarity class.
  Finally, each similarity class contains exactly one matrix in rational canonical form by the same uniqueness property.
  Therefore it suffices to count the matrices in rational canonical form with the desired property.

  Therefore we need to count the possible $a_1(x),\dots,a_m(x)$ satisfying:
  \begin{itemize}
  \item $a_1(x) | a_2(x) | \dots | a_m(x)$
  \item $a_1(x) a_2(x) \dots a_m(x) = \det(xI - T) = x^8(x+1)^4(x+2)^2$
  \item $a_m(x) = m_T(x) = x^5(x+1)^3(x+2)^2$
  \end{itemize}
  From the second two requirements, we have $a_1(x) \dots a_{m-1}(x) = x^3(x+1)$.
  Then by the divisibility requirement, we see that the only possibility is that $a_{m-1}(x)$ contains the linear factor $(x+1)$.
  Finally, by the divisibility requirement we can either partition the $x^3$ as $x|x|x$, $x|x^2$, or $x^3$.
  Therefore we only have one choice to make with $3$ options, and conclude there are $3$ distinct similarity classes.  
\end{proof}

\problem{4}
First we calculate the smith normal form of the matrix, which will be used to compute all the other values:
\begin{equation*}
  \begin{bmatrix}
    6 & -3 & -3 \\
    -3 & 6 & -3 \\
    -3 & -3 & 6
  \end{bmatrix}  
  \xrightarrow{\text{swap rows 1 and 2}}
  \begin{bmatrix}
    -3 & 6 & -3 \\
    6 & -3 & -3 \\
    -3 & -3 & 6
  \end{bmatrix}
  \xrightarrow{\text{clear first column}}
  \begin{bmatrix}
    -3 & 6 & -3 \\
    0 & 9 & -9 \\
    0 & -9 & 9
  \end{bmatrix}
\end{equation*}
\begin{equation*}
  \hookrightarrow
  \begin{bmatrix}
    -3 & 6 & -3 \\
    0 & 9 & -9 \\
    0 & -9 & 9
  \end{bmatrix}
  \xrightarrow{\text{clear first row}}
  \begin{bmatrix}
    -3 & 0 & 0 \\
    0 & 9 & -9 \\
    0 & -9 & 9
  \end{bmatrix}
  \xrightarrow{\text{clear second column}}
  \begin{bmatrix}
    -3 & 0 & 0 \\
    0 & 9 & -9 \\
    0 & 0 & 0
  \end{bmatrix}
\end{equation*}
\begin{equation*}
  \hookrightarrow
  \begin{bmatrix}
    -3 & 0 & 0 \\
    0 & 9 & -9 \\
    0 & 0 & 0
  \end{bmatrix}
  \xrightarrow{\text{clear third column}}
  \begin{bmatrix}
    -3 & 0 & 0 \\
    0 & 9 & 0 \\
    0 & 0 & 0
  \end{bmatrix}
  \xrightarrow{\text{mutliply by unit -1}}
  \begin{bmatrix}
    3 & 0 & 0 \\
    0 & 9 & 0 \\
    0 & 0 & 0
  \end{bmatrix}
\end{equation*}

\subproblem{a: $\Z/3\Z \times \Z/9\Z \times \Z$}
\begin{proof}
  We saw in class that $\text{coker}(A) \cong \Z^3/\text{im}(A) \cong \Z^3/\text{im}(S)$ where $S$ is the smith normal form of $A$.
  Then $\text{im}(S)$ is the span of the columns of $S$, so $\text{im}(S) = 3\Z \times 9\Z \times 0\Z$.
  So finally we get that $\text{coker}(A) \cong \Z/3\Z \times \Z/9\Z \times \Z$
\end{proof}

\subproblem{b: $\Z$}
\begin{proof}
  We saw in the last part that the image has dimension $2$, so by rank-nullity the kernel has dimension $1$.
  Hence it must be a one-dimensional infinite cyclic group, and hence isomorphic to $\Z$, which is already in invariant factor form.
\end{proof}

\subproblem{c: $\Z^2$}
\begin{proof}
  By the first isomorphism theorem we have $\text{im}(A) \cong \Z^3/\text{ker}(A) = \Z^3/\Z \cong \Z^2$.
\end{proof}

\end{document}